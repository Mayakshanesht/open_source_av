# TASK 3: Paper Reading

## Action Items:
- Be part of one of these the group.
    - Group1: Lidar-Camera 
    - Group2: Radar-Camera
    - Group3: Lidar-Camera-Radar
- Find paper on each topics. Share it with the group so there are no conflicts.
- Follow [Tarun's example](https://docs.google.com/spreadsheets/d/1D0CSpxJMpQfcQ55jQGHQWEsWqWzX3jR_N69Lul2E9UI/edit?usp=sharing) and add details to the table he is preparing. 
- Be ready for next Saturday to discuss the details.

**Refer to the link to read research papers.**
> [Link to how to read Research paper](https://saiamrit.github.io/technical-blog/research/reading_papers/2021/07/31/read-papers.html)

**Review paper**
> https://www.mdpi.com/1424-8220/20/7/2068

# Groups:
Kindly be a part of one of the groups, and find papers on the topics.
## Group1: Lidar-Camera 
1. [Object detection lidar and camera](papers/Fusion%20of%203D%20LIDAR%20and%20Camera%20Data%20for%20Object%20Detection%20in%20Autono%20(1).pdf) - (Gautham Reviewed)
2. [A survey of LiDAR and camera fusion enhancement](papers/1-s2.0-S1877050921005767-main.pdf)  - (Gautham In progress)
3. [Fusion of LiDAR and camera sensor data for environment sensing in driverless vehicles](papers/Fusion_DeSilva_Double_Col.pdf)(Nawin Reading)
4. [Lidar-Camera Fusion for Road Detection Using Fully Convolutional Neural Networks](papers/LIDAR-Camera_Fusion_for_Road_Detection.pdf) (Milind reading)

## Group2: Radar-Camera
1. [Radar and Camera Early Fusion for Vehicle Detection in Advanced Driver Assistance Systems](papers/Radar%20and%20Camera%20Early%20Fusion%20for%20Vehicle%20Detection%20in%20Advanced%20Driver%20Assistance%20Systems.pdf)
2. [CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection](papers/Nabati_CenterFusion_Center-Based_Radar_and_Camera_Fusion_for_3D_Object_Detection_WACV_2021_paper.pdf)
3. [RCM-Fusion: Radar-Camera Multi-Level Fusion for 3D Object Detection](papers/2307.10249.pdf)

## Group3: Lidar-Camera-Radar
1. [FUTR3D: A Unified Sensor Fusion Framework for 3D Detection](papers/FUTR3D%20-%20A%20unified%20snsor%20fusion%20framework%20for%203D%20detection.pdf) - Mayank (reading)
2. [Camera, LiDAR, and Radar Sensor
Fusion Based on Bayesian Neural
Network (CLR-BNN)](papers/Camera_LiDAR_and_Radar_Sensor_Fusion_Based_on_Baye_240421_155603.pdf)

### Papers to read

1. [Bi-LRFusion: Bi-Directional LiDAR-Radar Fusion for 3D Dynamic Object Detection](papers/Wang_Bi-LRFusion_Bi-Directional_LiDAR-Radar_Fusion_for_3D_Dynamic_Object_Detection_CVPR_2023_paper.pdf)
2. [Surrounding Objects Detection and Tracking for Autonomous Driving Using LiDAR and Radar Fusion](papers/s10033-021-00630-y.pdf)
3. [Ego-Motion Estimation and Dynamic Motion Separation from 3D Point](https://arxiv.org/pdf/2308.15357.pdf)
4. [Deep LiDAR-Radar-Visual Fusion for Object Detection in Urban Environments](https://www.mdpi.com/2072-4292/15/18/4433)
5. [Transformer-Based Sensor Fusion for Autonomous Driving: A Survey](papers/Singh_Transformer-Based_Sensor_Fusion_for_Autonomous_Driving_A_Survey_ICCVW_2023_paper_240421_140750.pdf)

## PS
After reading the paper kindly add the details to the sheet shared by Tarun.

> [Google Sheet](https://docs.google.com/spreadsheets/d/1D0CSpxJMpQfcQ55jQGHQWEsWqWzX3jR_N69Lul2E9UI/edit?usp=sharing)

1. Problem statement the paper has addressed
2. Aim of the paper
3. Approach used to address the problem
4. Pipeline of implementation (experiment)
5. Algorithm used - backbone and main network 
6. Note down the accuracy of the algorithm, how can you increase the accuracy of the algorithm
